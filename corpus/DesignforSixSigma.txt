design for six sigma dfss is a business-process management methodology related to traditional six sigma it is used in many industries like finance marketing basic engineering process industries waste management and electronics it is based on the use of statistical tools like linear regression and enables empirical research similar to that performed in other fields such as social science while the tools and order used in six sigma require a process to be in place and functioning dfss has the objective of determining the needs of customers and the business and driving those needs into the product solution so created dfss is relevant for relatively simple items  systems it is used for product or process design in contrast with process improvement measurement is the most important part of most six sigma or dfss tools but whereas in six sigma measurements are made from an existing process dfss focuses on gaining a deep insight into customer needs and using these to inform every design decision and trade-off
there are different options for the implementation of dfss unlike six sigma which is commonly driven via dmaic define - measure - analyze - improve - control projects dfss has spawned a number of stepwise processes all in the style of the dmaic procedure
dmadv define – measure – analyze – design – verify is sometimes synonymously referred to as dfss although alternatives such as idov identify design optimize verify are also used the traditional dmaic six sigma process as it is usually practiced which is focused on evolutionary and continuous improvement manufacturing or service process development usually occurs after initial system or product design and development have been largely completed dmaic six sigma as practiced is usually consumed with solving existing manufacturing or service process problems and removal of the defects and variation associated with defects it is clear that manufacturing variations may impact product reliability so a clear link should exist between reliability engineering and six sigma quality in contrast dfss or dmadv and idov strives to generate a new process where none existed or where an existing process is deemed to be inadequate and in need of replacement dfss aims to create a process with the end in mind of optimally building the efficiencies of six sigma methodology into the process before implementation traditional six sigma seeks for continuous improvement after a process already exists


dfss seeks to avoid manufacturingservice process problems by using advanced techniques to avoid process problems at the outset eg fire prevention when combined these methods obtain the proper needs of the customer and derive engineering system parameter requirements that increase product and service effectiveness in the eyes of the customer and all other people this yields products and services that provide great customer satisfaction and increased market share these techniques also include tools and processes to predict model and simulate the product delivery system the processestools personnel and organization training facilities and logistics to produce the productservice in this way dfss is closely related to operations research solving the knapsack problem workflow balancing dfss is largely a design activity requiring tools including quality function deployment qfd axiomatic design triz design for x design of experiments doe taguchi methods tolerance design robustification and response surface methodology for a single or multiple response optimization while these tools are sometimes used in the classic dmaic six sigma process they are uniquely used by dfss to analyze new and unprecedented products and processes it is a concurrent analyzes directed to manufacturing optimization related to the design
response surface methodology and other dfss tools uses statistical often empirical models and therefore practitioners need to be aware that even the best statistical model is an approximation to reality in practice both the models and the parameter values are unknown and subject to uncertainty on top of ignorance of course an estimated optimum point need not be optimum in reality because of the errors of the estimates and of the inadequacies of the model
nonetheless response surface methodology has an effective track-record of helping researchers improve products and services for example george box's original response-surface modeling enabled chemical engineers to improve a process that had been stuck at a saddle-point for yearscitation needed
proponents of dmaic ddica design develop initialize control and allocate and lean techniques might claim that dfss falls under the general rubric of six sigma or lean six sigma lss both methodologies focus on meeting customer needs and business priorities as the starting-point for analysis
it is often seen that the tools used for dfss techniques vary widely from those used for dmaic six sigma in particular dmaic ddica practitioners often use new or existing mechanical drawings and manufacturing process instructions as the originating information to perform their analysis while dfss practitioners often use simulations and parametric system designanalysis tools to predict both cost and performance of candidate system architectures while it can be claimed that two processes are similar in practice the working medium differs enough so that dfss requires different tool sets in order to perform its design tasks dmaic idov and six sigma may still be used during depth-first plunges into the system architecture analysis and for back end six sigma processes dfss provides system design processes used in front-end complex system designs back-front systems also are used this makes 3.4 defects per million design opportunities if done well
traditional six sigma methodology dmaic has become a standard process optimization tool for the chemical process industries however it has become clear that the promise of six sigma specifically 3.4 defects per million opportunities dpmo is simply unachievable after the fact consequentlythere has been a growing movement to implement six sigma design usually called design for six sigma dfss and ddica tools this methodology begins with defining customer needs and leads to the development of robust processes to deliver those needs
design for six sigma emerged from the six sigma and the define-measure-analyze-improve-control dmaic quality methodologies which were originally developed by motorola to systematically improve processes by eliminating defects unlike its traditional six sigmadmaic predecessors which are usually focused on solving existing manufacturing issues ie fire fighting dfss aims at avoiding manufacturing problems by taking a more proactive approach to problem solving and engaging the company efforts at an early stage to reduce problems that could occur ie fire prevention the primary goal of dfss is to achieve a significant reduction in the number of nonconforming units and production variation it starts from an understanding of the customer expectations needs and critical to quality issues ctqs before a design can be completed typically in a dfss program only a small portion of the ctqs are reliability-related ctr and therefore reliability does not get center stage attention in dfss dfss rarely looks at the long-term after manufacturing issues that might arise in the product eg complex fatigue issues or electrical wear-out chemical issues cascade effects of failures system level interactions
arguments about what makes dfss different from six sigma demonstrate the similarities between dfss and other established engineering practices such as probabilistic design and design for quality in general six sigma with its dmaic roadmap focuses on improvement of an existing process or processes dfss focuses on the creation of new value with inputs from customers suppliers and business needs while traditional six sigma may also use those inputs the focus is again on improvement and not design of some new product or system it also shows the engineering background of dfss however like other methods developed in engineering there is no theoretical reason why dfss cannot be used in areas outside of engineering
historically although the first successful design for six sigma projects in 1989 and 1991 predate establishment of the dmaic process improvement process design for six sigma dfss is accepted in part because six sigma organisations found that they could not optimise products past three or four sigma without fundamentally redesigning the product and because improving a process or product after launch is considered less efficient and effective than designing in quality ‘six sigma’ levels of performance have to be ‘built-in’
dfss for software is essentially a non superficial modification of classical dfss since the character and nature of software is different from other fields of engineering the methodology describes the detailed process for successfully applying dfss methods and tools throughout the software product design covering the overall software development life cycle requirements architecture design implementation integration optimization verification and validation radiov the methodology explains how to build predictive statistical models for software reliability and robustness and shows how simulation and analysis techniques can be combined with structural design and architecture methods to effectively produce software and information systems at six sigma levels
dfss in software acts as a glue to blend the classical modelling techniques of software engineering such as object-oriented design or evolutionary rapid development with statistical predictive models and simulation techniques the methodology provides software engineers with practical tools for measuring and predicting the quality attributes of the software product and also enables them to include software in system reliability models
although many tools used in dfss consulting such as response surface methodology transfer function via linear and non linear modeling axiomatic design simulation have their origin in inferential statistics statistical modeling may overlap with data analytics and mining
however despite that dfss as a methodology has been successfully used as an end-to-end technical project frameworks  for analytic and mining projects this has been observed by domain experts to be some what similar to the lines of crisp-dm
dfss is claimed to be better suited for encapsulating and effectively handling higher number of uncertainties including missing and uncertain data both in terms of acuteness of definition and their absolute total numbers with respect to analytic s and data-mining tasks six sigma approaches to data-mining are popularly known as dfss over crisp  crisp- dm referring to data-mining application framework methodology of spss 
with dfss data mining projects have been observed to have considerably shortened development life cycle  this is typically achieved by conducting data analysis to pre-designed template match tests via a techno-functional approach using multilevel quality function deployment on the data-set
practitioners claim that progressively complex kdd templates are created by multiple doe runs on simulated complex multivariate data then the templates along with logs are extensively documented via a decision tree based algorithm
dfss uses quality function deployment and sipoc for feature engineering of known independent variables thereby aiding in techno-functional computation of derived attributes
once the predictive model has been computed dfss studies can also be used to provide stronger probabilistic estimations of predictive model rank in a real world scenario
dfss framework has been successfully applied for predictive analytics pertaining to the hr analytics field this application field has been considered to be traditionally very challenging due to the peculiar complexities of predicting human behavior
