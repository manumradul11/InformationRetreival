reliability engineering is engineering that emphasizes dependability in the lifecycle management of a product dependability or reliability describes the ability of a system or component to function under stated conditions for a specified period of time reliability may also describe the ability to function at a specified moment or interval of time availability reliability engineering represents a sub-discipline within systems engineering reliability is theoretically defined as the probability of success reliability=1-probability of failure as the frequency of failures or in terms of availability as a probability derived from reliability testability and maintainability testability maintainability and maintenance are often defined as a part of reliability engineering in reliability programs reliability plays a key role in the cost-effectiveness of systems
reliability engineering deals with the estimation prevention and management of high levels of lifetime engineering uncertainty and risks of failure although stochastic parameters define and affect reliability according to some expert authors on reliability engineering eg p o'conner j moubray and a barnard reliability is not solely achieved by mathematics and statistics you cannot really find a root cause needed to effectively prevent failures by only looking at statistics nearly all teaching and literature on the subject emphasize these aspects and ignore the reality that the ranges of uncertainty involved largely invalidate quantitative methods for prediction and measurement 
reliability engineering relates closely to safety engineering and to system safety in that they use common methods for their analysis and may require input from each other reliability engineering focuses on costs of failure caused by system downtime cost of spares repair equipment personnel and cost of warranty claims safety engineering normally emphasizes not cost but preserving life and nature and therefore deals only with particular dangerous system-failure modes high reliability safety factor levels also result from good engineering and from attention to detail and almost never from only reactive failure management reliability accounting  statistics
a former united states secretary of defense economist james r schlesinger once stated reliability is after all engineering in its most practical form


the word reliability can be traced back to 1816 by poet coleridge before world war ii the name has been linked mostly to repeatability a test in any type of science was considered reliable if the same results would be obtained repeatedly in the 1920s product improvement through the use of statistical process control was promoted by dr walter a shewart at bell labs around this time wallodi weibull was working on statistical models for fatigue the development of reliability engineering was here on a parallel path with quality the modern use of the word reliability was defined by the us military in the 1940s and evolved to the present it initially came to mean that a product would operate when expected nowadays called mission readiness and for a specified period of time in the time around the wwii and later many reliability issues were due to inherent unreliability of electronics and to fatigue issues in 1945 ma miner published the seminal paper titled cumulative damage in fatigue in an asme journal a main application for reliability engineering in the military was for the vacuum tube as used in radar systems and other electronics for which reliability has proved to be very problematic and costly the ieee formed the reliability society in 1948 in 1950 on the military side a group called the advisory group on the reliability of electronic equipment agree was born this group recommended the following 3 main ways of working
in the 1960s more emphasis was given to reliability testing on component and system level the famous military standard 781 was created at that time around this period also the much-used and also much-debated military handbook 217 was published by rca radio corporation of america and was used for the prediction of failure rates of components the emphasis on component reliability and empirical research eg mil std 217 alone slowly decreases more pragmatic approaches as used in the consumer industries are being used the 1980s was a decade of great changes televisions had become all semiconductor automobiles rapidly increased their use of semiconductors with a variety of microcomputers under the hood and in the dash large air conditioning systems developed electronic controllers as had microwave ovens and a variety of other appliances communications systems began to adopt electronics to replace older mechanical switching systems bellcore issued the first consumer prediction methodology for telecommunications and sae developed a similar document sae870050 for automotive applications the nature of predictions evolved during the decade and it became apparent that die complexity wasn't the only factor that determined failure rates for integrated circuits ics kam wong published a paper questioning the bathtub curve —see also reliability centered maintenance during this decade the failure rate of many components dropped by a factor of 10 software became important to the reliability of systems by the 1990s the pace of ic development was picking up wider use of stand-alone microcomputers was common and the pc market helped keep ic densities following moore’s law and doubling about every 18 months reliability engineering now was more changing towards understanding the physics of failure failure rates for components kept on dropping but system-level issues became more prominent systems thinking became more and more important for software the ccm model capability maturity model was developed which gave a more qualitative approach to reliability iso 9000 added reliability measures as part of the design and development portion of certification the expansion of the world-wide web created new challenges of security and trust the older problem of too little reliability information available had now been replaced by too much information of questionable value consumer reliability problems could now have data and be discussed online in real time new technologies such as micro-electromechanical systems mems handheld gps and hand-held devices that combined cell phones and computers all represent challenges to maintain reliability product development time continued to shorten through this decade and what had been done in three years was being done in 18 months this meant that reliability tools and tasks must be more closely tied to the development process itself in many ways reliability became part of everyday life and consumer expectations
the objectives of reliability engineering in decreasing order of priority are
the reason for the priority emphasis is that it is by far the most effective way of working in terms of minimizing costs and generating reliable productsthe primary skills that are required therefore are the ability to understand and anticipate the possible causes of failures and knowledge of how to prevent them it is also necessary to have knowledge of the methods that can be used for analysing designs and data
reliability engineering for complex systems requires a different more elaborate systems approach than for non-complex systems reliability engineering may in that case involve
effective reliability engineering requires understanding of the basics of failure mechanisms for which experience broad engineering skills and good knowledge from many different special fields of engineering like
reliability may be defined in the following ways
many engineering techniques are used in reliability risk assessments such as reliability hazard analysis failure mode and effects analysis fmea fault tree analysis fta reliability centered maintenance probabilistic load and material stress and wear calculations probabilistic fatigue and creep analysis human error analysis manufacturing defect analysis reliability testing etc it is crucial that these analysis are done properly and with much attention to detail to be effective because of the large number of reliability techniques their expense and the varying degrees of reliability required for different situations most projects develop a reliability program plan to specify the reliability tasks sow requirements that will be performed for that specific system
consistent with the creation of a safety cases for example arp4761 the goal of reliability assessments is to provide a robust set of qualitative and quantitative evidence that use of a component or system will not be associated with unacceptable risk the basic steps to take are to
risk is here the combination of probability and severity of the failure incident scenario occurring
in a deminimus definition severity of failures include the cost of spare parts man-hours logistics damage secondary failures and downtime of machines which may cause production loss a more complete definition of failure also can mean injury dismemberment and death of people within the system witness mine accidents industrial accidents space shuttle failures and the same to innocent bystanders witness the citizenry of cities like bhopal love canal chernobyl or sendai and other victims of the 2011 tōhoku earthquake and tsunami--in this case reliability engineering becomes system safety what is acceptable is determined by the managing authority or customers or the affected communities residual risk is the risk that is left over after all reliability activities have finished and includes the un-identified risk—and is therefore not completely quantifiable
the complexity of the technical systems such as improvements of design and materials planned inspections fool-proof design and backup redundancy decreases risk and increases the cost the risk can be decreased to alara as low as reasonably achievable or alapa as low as practically achievable levels
implementing a reliability program is not simply a software purchase it’s not just a checklist of items that must be completed that will ensure you have reliable products and processes a reliability program is a complex learning and knowledge-based system unique to your products and processes it is supported by leadership built on the skills that you develop within your team integrated into your business processes and executed by following proven standard work practices 
a reliability program plan is used to document exactly what best practices tasks methods tools analysis and tests are required for a particular subsystem as well as clarify customer requirements for reliability assessment for large-scale complex systems the reliability program plan should be a separate document resource determination for manpower and budgets for testing and other tasks is critical for a successful program in general the amount of work required for an effective program for complex systems is large
a reliability program plan is essential for achieving high levels of reliability testability maintainability and the resulting system availability and is developed early during system development and refined over the system's life-cycle it specifies not only what the reliability engineer does but also the tasks performed by other stakeholders a reliability program plan is approved by top program management which is responsible for allocation of sufficient resources for its implementation
a reliability program plan may also be used to evaluate and improve availability of a system by the strategy of focusing on increasing testability & maintainability and not on reliability improving maintainability is generally easier than improving reliability maintainability estimates repair rates are also generally more accurate however because the uncertainties in the reliability estimates are in most cases very large they are likely to dominate the availability calculation prediction uncertainty problem even when maintainability levels are very high when reliability is not under control more complicated issues may arise like manpower maintainers  customer service capability shortages spare part availability logistic delays lack of repair facilities extensive retro-fit and complex configuration management costs and others the problem of unreliability may be increased also due to the domino effect of maintenance-induced failures after repairs focusing only on maintainability is therefore not enough if failures are prevented none of the other issues are of any importance and therefore reliability is generally regarded as the most important part of availability reliability needs to be evaluated and improved related to both availability and the total cost of ownership tco due to cost of spare parts maintenance man-hours transport costs storage cost part obsolete risks etc but as gm and toyota have belatedly discovered tco also includes the downstream liability costs when reliability calculations have not sufficiently or accurately addressed customers' personal bodily risks often a trade-off is needed between the two there might be a maximum ratio between availability and cost of ownership testability of a system should also be addressed in the plan as this is the link between reliability and maintainability the maintenance strategy can influence the reliability of a system eg by preventive andor predictive maintenance although it can never bring it above the inherent reliability
the reliability plan should clearly provide a strategy for availability control whether only availability or also cost of ownership is more important depends on the use of the system for example a system that is a critical link in a production system–-eg a big oil platform—is normally allowed to have a very high cost of ownership if that cost translates to even a minor increase in availability as the unavailability of the platform results in a massive loss of revenue which can easily exceed the high cost of ownership a proper reliability plan should always address ramt analysis in its total context ramt stands for reliability availability maintainabilitymaintenance and testability in context to the customer needs
for any system one of the first tasks of reliability engineering is to adequately specify the reliability and maintainability requirements allocated from the overall availability needs and more importantly derived from proper design failure analysis or preliminary prototype test results clear requirements able to designed to should constrain the designers from designing particular unreliable items  constructions  interfaces  systems setting only availability reliability testability or maintainability targets eg max failure rates is not appropriate this is a broad misunderstanding about reliability requirements engineering reliability requirements address the system itself including test and assessment requirements and associated tasks and documentation reliability requirements are included in the appropriate system or subsystem requirements specifications test plans and contract statements creation of proper lower-level requirements is critical provision of only quantitative minimum targets eg mtbf values or failure rates is not sufficient for different reasons one reason is that a full validation related to correctness and verifiability in time of an quantitative reliability allocation requirement spec on lower levels for complex systems can often not be made as a consequence of 1 the fact that the requirements are probabalistic 2 the extremely high level of uncertainties involved for showing compliance with all these probabalistic requirements and because 3 reliability is a function of time and accurate estimates of a probabalistic reliability number per item are available only very late in the project sometimes even after many years of in-service use compare this problem with the continues re-balancing of for example lower-level-system mass requirements in the development of an aircraft which is already often a big undertaking notice that in this case masses do only differ in terms of only some % are not a function of time the data is non-probabalistic and available already in cad models in case of reliability the levels of unreliability failure rates may change with factors of decades multiples of 10 as result of very minor deviations in design process or anything else the information is often not available without huge uncertainties within the development phase this makes this allocation problem almost impossible to do in a useful practical valid manner that does not result in massive over- or under-specification a pragmatic approach is therefore needed—for example the use of general levels  classes of quantitative requirements depending only on severity of failure effects also the validation of results is a far more subjective task than for any other type of requirement quantitative reliability parameters—in terms of mtbf—are by far the most uncertain design parameters in any design
furthermore reliability design requirements should drive a system or part design to incorporate features that prevent failures from occurring or limit consequences from failure in the first place not only would it aid in some predictions this effort would keep from distracting the engineering effort into a kind of accounting work a design requirement should be precise enough so that a designer can design to it and can also prove—through analysis or testing—that the requirement has been achieved and if possible within some a stated confidence any type of reliability requirement should be detailed and could be derived from failure analysis finite-element stress and fatigue analysis reliability hazard analysis fta fmea human factor analysis functional hazard analysis etc or any type of reliability testing also requirements are needed for verification tests eg required overload stresses and test time needed to derive these requirements in an effective manner a systems engineering-based risk assessment and mitigation logic should be used robust hazard log systems must be created that contain detailed information on why and how systems could or have failed requirements are to be derived and tracked in this way these practical design requirements shall drive the design and not be used only for verification purposes these requirements often design constraints are in this way derived from failure analysis or preliminary tests understanding of this difference compared to only purely quantitative logistic requirement specification eg failure rate  mtbf target is paramount in the development of successful complex systems
the maintainability requirements address the costs of repairs as well as repair time testability not to be confused with test requirements requirements provide the link between reliability and maintainability and should address detectability of failure modes on a particular system level isolation levels and the creation of diagnostics procedures as indicated above reliability engineers should also address requirements for various reliability tasks and documentation during system development testing production and operation these requirements are generally specified in the contract statement of work and depend on how much leeway the customer wishes to provide to the contractor reliability tasks include various analyses planning and failure reporting task selection depends on the criticality of the system as well as cost a safety-critical system may require a formal failure reporting and review process throughout development whereas a non-critical system may rely on final test reports the most common reliability program tasks are documented in reliability program standards such as mil-std-785 and ieee 1332 failure reporting analysis and corrective action systems are a common approach for productprocess reliability monitoring
practically most failures can in the end be traced back to a root causes of the type of human error of any kind for example human errors in
however humans are also very good in detection of the same failures correction of failures and improvising when abnormal situations occur the policy that human actions should be completely ruled out of any design and production process to improve reliability may not be effective therefore some tasks are better performed by humans and some are better performed by machines
furthermore human errors in management and the organization of data and information or the misuse or abuse of items may also contribute to unreliability this is the core reason why high levels of reliability for complex systems can only be achieved by following a robust systems engineering process with proper planning and execution of the validation and verification tasks this also includes careful organization of data and information sharing and creating a reliability culture in the same sense as having a safety culture is paramount in the development of safety critical systems
reliability prediction is the combination of the creation of a proper reliability model see further on this page together with estimating and justifying the input parameters for this model like failure rates for a particular failure mode or event and the mean time to repair the system for a particular failure and finally to provide a system or part level estimate for the output reliability parameters system availability or a particular functional failure frequency
some recognized reliability engineering specialists – eg patrick o'connor r barnard – have argued that too much emphasis is often given to the prediction of reliability parameters and more effort should be devoted to the prevention of failure reliability improvement failures can and should be prevented in the first place for most cases the emphasis on quantification and target setting in terms of eg mtbf might provide the idea that there is a limit to the amount of reliability that can be achieved in theory there is no inherent limit and higher reliability does not need to be more costly in development another of their arguments is that prediction of reliability based on historic data can be very misleading as a comparison is only valid for exactly the same designs products manufacturing processes and maintenance under exactly the same loads and environmental context even a minor change in detail in any of these could have major effects on reliability furthermore normally the most unreliable and important items most interesting candidates for a reliability investigation are most often subjected to many modifications and changes engineering designs are in most industries updated frequently this is the reason why the standard re-active or pro-active statistical methods and processes as used in the medical industry or insurance branch are not as effective for engineering another surprising but logical argument is that to be able to accurately predict reliability by testing the exact mechanisms of failure must have been known in most cases and therefore – in most cases – can be prevented following the incorrect route by trying to quantify and solving a complex reliability engineering problem in terms of mtbf or probability and using the re-active approach is referred to by barnard as playing the numbers game and is regarded as bad practise
for existing systems it is arguable that responsible programs would directly analyse and try to correct the root cause of discovered failures and thereby may render the initial mtbf estimate fully invalid as new assumptions subject to high error levels of the effect of the patchredesign must be made another practical issue concerns a general lack of availability of detailed failure data and not consistent filtering of failure feedback data or ignoring statistical errors which are very high for rare events like reliability related failures very clear guidelines must be present to be able to count and compare failures related to different type of root-causes eg manufacturing- maintenance- transport- system-induced or inherent design failures  comparing different type of causes may lead to incorrect estimations and incorrect business decisions about the focus of improvement
to perform a proper quantitative reliability prediction for systems may be difficult and may be very expensive if done by testing on part level results can be obtained often with higher confidence as many samples might be used for the available testing financial budget however unfortunately these tests might lack validity on system level due to the assumptions that had to be made for part level testing these authors argue that it can not be emphasized enough that testing for reliability should be done to create failures in the first place learn from them and to improve the system  part the general conclusion is drawn that an accurate and an absolute prediction – by field data comparison or testing – of reliability is in most cases not possible an exception might be failures due to wear-out problems like fatigue failures in the introduction of mil-std-785 it is written that reliability prediction should be used with great caution if not only used for comparison in trade-off studies
see also risk assessment#quantitative risk assessment – critics paragraph
reliability design begins with the development of a system model reliability and availability models use block diagrams and fault tree analysis to provide a graphical means of evaluating the relationships between different parts of the system these models may incorporate predictions based on failure rates taken from historical data while the input data predictions are often not accurate in an absolute sense they are valuable to assess relative differences in design alternatives maintainability parameters for example mttr are other inputs for these models
the most important fundamental initiating causes and failure mechanisms are to be identified and analyzed with engineering tools a diverse set of practical guidance and practical performance and reliability requirements should be provided to designers so they can generate low-stressed designs and products that protect or are protected against damage and excessive wear proper validation of input loads requirements may be needed and verification for reliability performance by testing may be needed
one of the most important design techniques is redundancy this means that if one part of the system fails there is an alternate success path such as a backup system the reason why this is the ultimate design choice is related to the fact that high confidence reliability evidence for new parts  items is often not available or extremely expensive to obtain by creating redundancy together with a high level of failure monitoring and the avoidance of common cause failures even a system with relative bad single channel part reliability can be made highly reliable mission reliability on system level no testing of reliability has to be required for this furthermore by using redundancy and the use of dissimilar design and manufacturing processes different suppliers for the single independent channels less sensitivity for quality issues early childhood failures is created and very high levels of reliability can be achieved at all moments of the development cycles early life times and long term redundancy can also be applied in systems engineering by double checking requirements data designs calculations software and tests to overcome systematic failures
another design technique to prevent failures is called physics of failure this technique relies on understanding the physical static and dynamic failure mechanisms it accounts for variation in load strength and stress leading to failure at high level of detail possible with use of modern finite element method fem software programs that may handle complex geometries and mechanisms like creep stress relaxation fatigue and probabilistic design monte carlo simulations  doe the material or component can be re-designed to reduce the probability of failure and to make it more robust against variation another common design technique is component derating selecting components whose tolerance significantly exceeds the expected stress as using a heavier gauge wire that exceeds the normal specification for the expected electric current
another effective way to deal with unreliability issues is to perform analysis to be able to predict degradation and being able to prevent unscheduled down events  failures from occurring rcm reliability centered maintenance programs can be used for this
many tasks techniques and analyses are specific to particular industries and applications commonly these include
results are presented during the system design reviews and logistics reviews reliability is just one requirement among many system requirements engineering trade studies are used to determine the optimum balance between reliability and other requirements and constraints
reliability engineers could concentrate more on why and how items  systems may fail or have failed instead of mostly trying to predict when or at what changing rate failure rate t answers to the first questions will drive improvement in design and processes when failure mechanisms are really understood then solutions to prevent failure are easily found only required numbers eg mtbf will not drive good designs the huge amount of unreliability hazards that are generally part of complex systems need first to be classified and ordered based on qualitative and quantitative logic if possible to get to efficient assessment and improvement this is partly done in pure language and proposition logic but also based on experience with similar items this can for example be seen in descriptions of events in fault tree analysis fmea analysis and a hazard tracking log in this sense language and proper grammar part of qualitative analysis plays an important role in reliability engineering just like it does in safety engineering or in general within systems engineering engineers are likely to question why? well it is precisely needed because systems engineering is very much about finding the correct words to describe the problem and related risks to be solved by the engineering solutions we intend to create in the words of jack ring the systems engineer’s job is to language the project ring et al 2000 language in itself is about putting an order in a description of the reality of a failure of a complex functionitemsystem in a complex surrounding reliability engineers use both quantitative and qualitative methods which extensively use language to pinpoint the risks to be solved
the importance of language also relates to the risks of human error which can be seen as the ultimate root cause of almost all failures - see further on this site as an example proper instructions often written by technical authors in so called simplified english in maintenance manuals operation manuals emergency procedures and others are needed to prevent systematic human errors in any maintenance or operational task that may result in system failures
reliability modeling is the process of predicting or understanding the reliability of a component or system prior to its implementation two types of analysis that are often used to model a complete system availability including effects from logistics issues like spare part provisioning transport and manpower behavior are fault tree analysis and reliability block diagrams on component level the same type of analysis can be used together with others the input for the models can come from many sources testing earlier operational experience field data or data handbooks from the same or mixed industries can be used in all cases the data must be used with great caution as predictions are only valid in case the same product in the same context is used often predictions are only made to compare alternatives
for part level predictions two separate fields of investigation are common
software reliability is a more challenging area that must be considered when it is a considerable component to system functionality
reliability is defined as the probability that a device will perform its intended function during a specified period of time under stated conditions mathematically this may be expressed as
there are a few key elements of this definition
quantitative requirements are specified using reliability parameters the most common reliability parameter is the mean time to failure mttf which can also be specified as the failure rate this is expressed as a frequency or conditional probability density function pdf or the number of failures during a given period these parameters may be useful for higher system levels and systems that are operated frequently such as most vehicles machinery and electronic equipment reliability increases as the mttf increases the mttf is usually specified in hours but can also be used with other units of measurement such as miles or cycles using mttf values on lower system levels can be very misleading specially if the failures modes and mechanisms it concerns the f in mttf are not specified with it
in other cases reliability is specified as the probability of mission success for example reliability of a scheduled aircraft flight can be specified as a dimensionless probability or a percentage as in system safety engineering
a special case of mission success is the single-shot device or system these are devices or systems that remain relatively dormant and only operate once examples include automobile airbags thermal batteries and missiles single-shot reliability is specified as a probability of one-time success or is subsumed into a related parameter single-shot missile reliability may be specified as a requirement for the probability of a hit for such systems the probability of failure on demand pfd is the reliability measure – which actually is an unavailability number this pfd is derived from failure rate a frequency of occurrence and mission time for non-repairable systems
for repairable systems it is obtained from failure rate and mean-time-to-repair mttr and test interval this measure may not be unique for a given system as this measure depends on the kind of demand in addition to system level requirements reliability requirements may be specified for critical subsystems in most cases reliability parameters are specified with appropriate statistical confidence intervals
the purpose of reliability testing is to discover potential problems with the design as early as possible and ultimately provide confidence that the system meets its reliability requirements
reliability testing may be performed at several levels and there are different types of testing complex systems may be tested at component circuit board unit assembly subsystem and system levels    the test level nomenclature varies among applications for example performing environmental stress screening tests at lower levels such as piece parts or small assemblies catches problems before they cause failures at higher levels testing proceeds during each level of integration through full-up system testing developmental testing and operational testing thereby reducing program risk however testing does not mitigate unreliability risk
with each test both a statistical type 1 and type 2 error could be made and depends on sample size test time assumptions and the needed discrimination ratio there is risk of incorrectly accepting a bad design type 1 error and the risk of incorrectly rejecting a good design type 2 error
it is not always feasible to test all system requirements some systems are prohibitively expensive to test some failure modes may take years to observe some complex interactions result in a huge number of possible test cases and some tests require the use of limited test ranges or other resources in such cases different approaches to testing can be used such as highly accelerated life testing design of experiments and simulations
the desired level of statistical confidence also plays an role in reliability testing statistical confidence is increased by increasing either the test time or the number of items tested reliability test plans are designed to achieve the specified reliability at the specified confidence level with the minimum number of test units and test time different test plans result in different levels of risk to the producer and consumer the desired reliability statistical confidence and risk levels for each side influence the ultimate test plan the customer and developer should agree in advance on how reliability requirements will be tested
a key aspect of reliability testing is to define failure although this may seem obvious there are many situations where it is not clear whether a failure is really the fault of the system variations in test conditions operator differences weather and unexpected situations create differences between the customer and the system developer one strategy to address this issue is to use a scoring conference process a scoring conference includes representatives from the customer the developer the test organization the reliability organization and sometimes independent observers the scoring conference process is defined in the statement of work each test case is considered by the group and scored as a success or failure this scoring is the official result used by the reliability engineer
as part of the requirements phase the reliability engineer develops a test strategy with the customer the test strategy makes trade-offs between the needs of the reliability organization which wants as much data as possible and constraints such as cost schedule and available resources test plans and procedures are developed for each reliability test and results are documented
reliability testing is common in the photonics industry examples of reliability tests of lasers are life test and burn-in these tests consist of the highly accelerated ageing under controlled conditions of a group of lasers the data collected from these life tests are used to predict laser life expectancy under the intended operating characteristics
reliability test requirements can follow from any analysis for which the first estimate of failure probability failure mode or effect needs to be justified evidence can be generated with some level of confidence by testing with software-based systems the probability is a mix of software and hardware-based failures testing reliability requirements is problematic for several reasons a single test is in most cases insufficient to generate enough statistical data multiple tests or long-duration tests are usually very expensive some tests are simply impractical and environmental conditions can be hard to predict over a systems life-cycle
reliability engineering is used to design a realistic and affordable test program that provides empirical evidence that the system meets its reliability requirements statistical confidence levels are used to address some of these concerns a certain parameter is expressed along with a corresponding confidence level for example an mtbf of 1000 hours at 90% confidence level from this specification the reliability engineer can for example design a test with explicit criteria for the number of hours and number of failures until the requirement is met or failed different sorts of tests are possible
the combination of required reliability level and required confidence level greatly affects the development cost and the risk to both the customer and producer care is needed to select the best combination of requirements – eg cost-effectiveness reliability testing may be performed at various levels such as component subsystem and system also many factors must be addressed during testing and operation such as extreme temperature and humidity shock vibration or other environmental factors like loss of signal cooling or power or other catastrophes such as fire floods excessive heat physical or security violations or other myriad forms of damage or degradation for systems that must last many years accelerated life tests may be needed
the purpose of accelerated life testing alt test is to induce field failure in the laboratory at a much faster rate by providing a harsher but nonetheless representative environment in such a test the product is expected to fail in the lab just as it would have failed in the field—but in much less time the main objective of an accelerated test is either of the following
an accelerated testing program can be broken down into the following steps
common way to determine a life stress relationship are
software reliability is a special aspect of reliability engineering system reliability by definition includes all parts of the system including hardware software supporting infrastructure including critical external interfaces operators and procedures traditionally reliability engineering focuses on critical hardware parts of the system since the widespread use of digital integrated circuit technology software has become an increasingly critical part of most electronics and hence nearly all present day systems
there are significant differences however in how software and hardware behave most hardware unreliability is the result of a component or material failure that results in the system not performing its intended function repairing or replacing the hardware component restores the system to its original operating state however software does not fail in the same sense that hardware fails instead software unreliability is the result of unanticipated results of software operations even relatively small software programs can have astronomically large combinations of inputs and states that are infeasible to exhaustively test restoring software to its original state only works until the same combination of inputs and states results in the same unintended result software reliability engineering must take this into account
despite this difference in the source of failure between software and hardware several software reliability models based on statistics have been proposed to quantify what we experience with software the longer software is run the higher the probability that it will eventually be used in an untested manner and exhibit a latent defect that results in a failure shooman 1987 musa 2005 denney 2005
as with hardware software reliability depends on good requirements design and implementation software reliability engineering relies heavily on a disciplined software engineering process to anticipate and design against unintended consequences there is more overlap between software quality engineering and software reliability engineering than between hardware quality and reliability a good software development plan is a key aspect of the software reliability program the software development plan describes the design and coding standards peer reviews unit tests configuration management software metrics and software models to be used during software development
a common reliability metric is the number of software faults usually expressed as faults per thousand lines of code this metric along with software execution time is key to most software reliability models and estimates the theory is that the software reliability increases as the number of faults or fault density decreases or goes down establishing a direct connection between fault density and mean-time-between-failure is difficult however because of the way software faults are distributed in the code their severity and the probability of the combination of inputs necessary to encounter the fault nevertheless fault density serves as a useful indicator for the reliability engineer other software metrics such as complexity are also used this metric remains controversial since changes in software development and verification practices can have dramatic impact on overall defect rates
testing is even more important for software than hardware even the best software development process results in some software faults that are nearly undetectable until tested as with hardware software is tested at several levels starting with individual units through integration and full-up system testing unlike hardware it is inadvisable to skip levels of software testing during all phases of testing software faults are discovered corrected and re-tested reliability estimates are updated based on the fault density and other metrics at a system level mean-time-between-failure data can be collected and used to estimate reliability unlike hardware performing exactly the same test on exactly the same software configuration does not provide increased statistical confidence instead software reliability uses different metrics such as code coverage
eventually the software is integrated with the hardware in the top-level system and software reliability is subsumed by system reliability the software engineering institute's capability maturity model is a common means of assessing the overall software development process for reliability and quality purposes
reliability engineering differs from safety engineering with respect to the kind of hazards that are considered reliability engineering is in the end only concerned with cost it relates to all reliability hazards that could transform into incidents with a particular level of loss of revenue for the company or the customer these can be cost due to loss of production due to system unavailability unexpected high or low demands for spares repair costs man hours multiple re-designs interruptions on normal production eg due to high repair times or due to unexpected demands for non-stocked spares and many other indirect costs
safety engineering on the other hand is more specific and regulated it relates to only very specific and system safety hazards that could potentially lead to severe accidents and is primarily concerned with loss of life loss of equipment or environmental damage the related system functional reliability requirements are sometimes extremely high it deals with unwanted dangerous events for life property and environment in the same sense as reliability engineering but does normally not directly look at cost and is not concerned with repair actions after failure  accidents on system level another difference is the level of impact of failures on society and the control of governments safety engineering is often strictly controlled by governments eg nuclear aerospace defense rail and oil industries
furthermore safety engineering and reliability engineering may even have contradicting requirements this relates to system level architecture choices citation needed for example in train signal control systems it is common practice to use a fail-safe system design concept in this concept the wrong-side failure need to be fully controlled to an extreme low failure rate these failures are related to possible severe effects like frontal collisions 2 green lights systems are designed in a way that the far majority of failures will simply result in a temporary or total loss of signals or open contacts of relays and generate red lights for all trains this is the safe state all trains are stopped immediately this fail-safe logic might unfortunately lower the reliability of the system the reason for this is the higher risk of false tripping as any full or temporary intermittent failure is quickly latched in a shut-down safestate different solutions are available for this issue see the section on fault tolerance below
reliability can be increased here by using a 2oo2 2 out of 2 redundancy on part or system level but this does in turn lower the safety levels more possibilities for wrong side and undetected dangerous failures fault tolerant voting systems eg 2oo3 voting logic can increase both reliability and safety on a system level in this case the so-called operational or mission reliability as well as the safety of a system can be increased this is also common practice in aerospace systems that need continued availability and do not have a fail-safe mode eg flight computers and related electrical and  or mechanical and  or hydraulic steering functions need always to be working there are no safe fixed positions for rudder or other steering parts when the aircraft is flying
the above example of a 2oo3 fault tolerant system increases both mission reliability as well as safety however the basic reliability of the system will in this case still be lower than a non redundant 1oo1 or 2oo2 system basic reliability refers to all failures including those that might not result in system failure but do result in maintenance repair actions logistic cost use of spares etc for example the replacement or repair of 1 channel in a 2oo3 voting system that is still operating with one failed channel which in this state actually has become a 1oo2 system is contributing to basic unreliability but not mission unreliability also for example the failure of the taillight of an aircraft is not considered as a mission loss failure but does contribute to the basic unreliability
when using fault tolerant redundant architectures systems or systems that are equipped with protection functions detectability of failures and avoidance of common cause failures becomes paramount for safe functioning andor mission reliability
six-sigma has its roots in manufacturing and reliability engineering is a sub-part of systems engineering the systems engineering process is a discovery process that is quite unlike a manufacturing process a manufacturing process is focused on repetitive activities that achieve high quality outputs with minimum cost and time the systems engineering process must begin by discovering the real potential problem that needs to be solved the biggest failure that can be made in systems engineering is finding an elegant solution to the wrong problem  or in terms of reliability providing elegant solutions to the wrong root causes of system failures
the everyday usage term quality of a product is loosely taken to mean its inherent degree of excellence in industry this is made more precise by defining quality to be conformance to requirement specifications at the start of use assuming the final product specifications adequately capture original requirements and customer or rest of system needs the quality level of these parts can now be precisely measured by the fraction of units shipped that meet the detailed product specifications
variation of this static output may affect quality and reliability but this is not the total picture more inherent aspects may play a role or variation at microscopic levels may not be controlled by any means eg one good example is the unavoidable existence of micro cracks and chemical impurities in standard metal products which may progress over time under physical or chemical loading into macro level defects furthermore on system level systematic failures may play a dominant role eg requirement errors or software or software compiler or design flaws
furthermore for more complex systems it should be questioned if derived lower level requirements and related product specifications are validated? will it later result in worn items and systems by general wear fatigue or corrosion mechanisms debris accumulation or due to maintenance induced failures? are there interactions on any system level as investigated by for example fault tree analysis? how many of these systems still meet function and fulfill the needs after a week of operation? what performance losses occurred? did full system failure occur? what happens after the end of a one-year warranty period? and what happens after 50 years a common lifetime for aircraft trains nuclear systems etc? that is where reliability comes in these issues are far more complex and can not be controlled only by a standard quality six sigma way of working they need a systems engineering approach
quality is a snapshot at the start of life and mainly related to control of lower level product specifications and reliability is as part of systems engineering more of a system level motion picture of the day-by-day operation for many years time zero defects are manufacturing mistakes that escaped final test quality control the additional defects that appear over time are reliability defects or reliability fallout these reliability issues may just as well occur due to inherent design issues which may have nothing to do with non-conformance product specifications items that are produced perfectly - according all product specifications - may fail over time due to any single or combined failure mechanism eg mechanical- electrical- chemical- or human error related all these parameters are also a function of all all possible variances coming from initial production theoretically all items will functionally fail over infinite time in theory the quality level might be described by a single fraction defective to describe reliability fallout a probability model that describes the fraction fallout over time is needed this is known as the life distribution model
quality is therefore related to manufacturing and reliability is more related to the validation of sub-system or lower item requirements system or part inherent design and life cycle solutions items that do not conform to any product specification in general will do worse in terms of reliability having a lower mttf but this does not always have to be the case the full mathematical quantification in statistical models of this combined relation is in general very difficult or even practical impossible in case manufacturing variances can be effectively reduced six sigma tools may be used to find optimal process solutions and may thereby also increase reliability six sigma may also help to design more robust related to manufacturing induced failures
in contrast with six sigma reliability engineering solutions are generally found by having a focus into a systemdesign and not on the manufacturing process solutions are found in different ways for example by simplifying a system and therefore understanding more mechanisms of failure involved detailed calculation of material stress levels and required safety factors finding possible abnormal system load conditions and next to this also to increase design robustness against variation from the manufacturing variances and related failure mechanisms furthermore reliability engineering use system level solutions like designing redundancy and fault tolerant systems in case of high availability needs see chapter reliability engineering vs safety engineering above
next to this and also in a major contrast with reliability engineering six-sigma is much more measurement based quantification the core of six-sigma thrives on empirical research and statistics where it is possible to measure parameters eg to find transfer functions this can not be translated practically to most reliability issues as reliability is not easy measurable due to the function of time large times may be involved specially during the requirements specification and design phase where reliability engineering is the most efficient full quantification of reliability is in this phase extremely difficult or costly testing it also may foster re-active management waiting for system failures to be measured furthermore as explained on this page reliability problems are likely to come from many different eg inherent failures human error systematic failures causes besides manufacturing induced defects
note what is called a defect however in six-sigma  quality literature is not the same as a failure field failure | eg fractured item in reliability a defects in six-sigma  quality refers generally to a non-conformance with a basis functional or dimensional requirement items can however fail over time even if these requirements eg a dimension are all fulfilled quality is normally not much concerned with the question if the requirements are correct
quality manufacturing six sigma processes and reliability design departments should provide input to each other to cover the complete risks more efficiently
after a system is produced reliability engineering monitors assesses and corrects deficiencies monitoring includes electronic and visual surveillance of critical parameters identified during the fault tree analysis design stage data collection is highly dependent on the nature of the system most large organizations have quality control groups that collect failure data on vehicles equipment and machinery consumer product failures are often tracked by the number of returns for systems in dormant storage or on standby it is necessary to establish a formal surveillance program to inspect and test random samples any changes to the system such as field upgrades or recall repairs require additional reliability testing to ensure the reliability of the modification since it is not possible to anticipate all the failure modes of a given system especially ones with a human element failures will occur the reliability program also includes a systematic root cause analysis that identifies the causal relationships involved in the failure such that effective corrective actions may be implemented when possible system failures and corrective actions are reported to the reliability engineering organization
one of the most common methods to apply to a reliability operational assessment are failure reporting analysis and corrective action systems fracas this systematic approach develops a reliability safety and logistics assessment based on failure  incident reporting management analysis and correctivepreventive actions organizations today are adopting this method and utilize commercial systems such as a web-based fracas application enabling an organization to create a failureincident data repository from which statistics can be derived to view accurate and genuine reliability safety and quality performances
it is extremely important to have one common source fracas system for all end items also test results should be able to be captured here in a practical way failure to adopt one easy to handle easy data entry for field engineers and repair shop engineersand maintain integrated system is likely to result in a fracas program failure
some of the common outputs from a fracas system includes field mtbf mttr spares consumption reliability growth failureincidents distribution by type location part no serial no symptom etc
the use of past data to predict the reliability of new comparable systemsitems can be misleading as reliability is a function of the context of use and can be affected by small changes in the designsmanufacturing
systems of any significant complexity are developed by organizations of people such as a commercial company or a government agency the reliability engineering organization must be consistent with the company's organizational structure for small non-critical systems reliability engineering may be informal as complexity grows the need arises for a formal reliability function because reliability is important to the customer the customer may even specify certain aspects of the reliability organization
there are several common types of reliability organizations the project manager or chief engineer may employ one or more reliability engineers directly in larger organizations there is usually a product assurance or specialty engineering organization which may include reliability maintainability quality safety human factors logistics etc in such case the reliability engineer reports to the product assurance manager or specialty engineering manager
in some cases a company may wish to establish an independent reliability organization this is desirable to ensure that the system reliability which is often expensive and time consuming is not unduly slighted due to budget and schedule pressures in such cases the reliability engineer works for the project day-to-day but is actually employed and paid by a separate organization within the company
because reliability engineering is critical to early system design it has become common for reliability engineers however the organization is structured to work as part of an integrated product team
some universities offer graduate degrees in reliability engineering other reliability engineers typically have an engineering degree which can be in any field of engineering from an accredited university or college program many engineering programs offer reliability courses and some universities have entire reliability engineering programs a reliability engineer may be registered as a professional engineer by the state but this is not required by most employers there are many professional conferences and industry training programs available for reliability engineers several professional organizations exist for reliability engineers including the ieee reliability society the american society for quality asq and the society of reliability engineers sre
a group of engineers have provided a list of useful tools for reliability engineering these include relcalc software military handbook 217 mil-hdbk-217 and the navmat p-4855-1a manual analyzing failures and successes coupled with a quality standards process also provides systemized information to making informed engineering designs
in the uk there are more up to date standards maintained under the sponsorship of uk mod as defence standards the relevant standards include
def stan 00-40 reliability and maintainability r&m
def stan 00-42 reliability and maintainability assurance guides
def stan 00-43 reliability and maintainability assurance activity
def stan 00-44 reliability and maintainability data collection and classification
def stan 00-45 issue 1 reliability centered maintenance
def stan 00-49 issue 1 reliability and maintainability mod guide to terminology definitions
these can be obtained from dstan there are also many commercial standards produced by many organisations including the sae msg arp and iee
